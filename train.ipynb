{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689ba92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 256, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efb2489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huseyin/Codes/llm.jarvis/.venv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are :\n",
      "tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "targets are :\n",
      "tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "source": [
    "from dataset import text_to_token_ids,token_ids_to_text\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1 = \"every effort moves\"\n",
    "text2 = \"I really like\"\n",
    "\n",
    "text1_encoded = text_to_token_ids(text1,tokenizer)\n",
    "text2_encoded = text_to_token_ids(text2,tokenizer)\n",
    "\n",
    "inputs = torch.vstack((text1_encoded,text2_encoded))\n",
    "print(\"inputs are :\")\n",
    "print(inputs)\n",
    "\n",
    "target1 =  \" effort moves you\"\n",
    "target2 = \" really like chocolate\"\n",
    "\n",
    "target1_encoded = text_to_token_ids(target1,tokenizer)\n",
    "target2_encoded = text_to_token_ids(target2,tokenizer)\n",
    "\n",
    "targets = torch.vstack((target1_encoded,target2_encoded))\n",
    "print(\"targets are :\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc8b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "from llms import GPT2\n",
    "model = GPT2(GPT_CONFIG_124M)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80ab9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[ 7001],\n",
      "         [24651],\n",
      "         [10481]],\n",
      "\n",
      "        [[16491],\n",
      "         [28345],\n",
      "         [17464]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b2bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Olymp sexist beach\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d4e6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([1.9182e-05, 1.1305e-05, 1.3430e-05])\n",
      "Text 2: tensor([2.4795e-05, 3.0446e-05, 2.1681e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be98542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.8615, -11.3903, -11.2180, -10.6049, -10.3996, -10.7391])\n",
      "tensor(10.8689)\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "avg_log_probas = torch.mean(log_probas) * -1 # -1 for NLL\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8beda51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ee3b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8689)\n",
      "tensor(52516.9766)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1787b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b6be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_point = int(0.9 * total_characters)\n",
    "\n",
    "data_train = text_data[:split_point]\n",
    "data_val = text_data[split_point:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2459a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloader\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader(\n",
    "    data_train,\n",
    "    batch_size=2,\n",
    "    max_len=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = create_dataloader(\n",
    "    data_val,\n",
    "    batch_size=2,\n",
    "    max_len=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b10a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c878642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch) #(batch_size,context_len,vocab_size)\n",
    "\n",
    "    # we will use torch.nn.functional \n",
    "    # we must do some dimensional arrangements\n",
    "    # target has size of (batch_size,context_len)\n",
    "    # logits has size of #(batch_size,context_len,vocab_size)\n",
    "    # We must transform targets to (batch_size * context_len)\n",
    "    # We must transform logits to (batch_size * context_len, vocab_size)\n",
    "    # Look Example small for cell below\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b13150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9447,  0.6217, -1.3501],\n",
      "         [-0.1881, -2.3891, -0.4759]],\n",
      "\n",
      "        [[ 1.7603,  0.6547,  0.5490],\n",
      "         [ 0.3671,  0.1219,  0.6466]]])\n",
      "-----------------------------\n",
      "tensor([[0, 1],\n",
      "        [0, 1]])\n",
      "tensor([0, 1, 0, 1])\n",
      "tensor([[ 0.9447,  0.6217, -1.3501],\n",
      "        [-0.1881, -2.3891, -0.4759],\n",
      "        [ 1.7603,  0.6547,  0.5490],\n",
      "        [ 0.3671,  0.1219,  0.6466]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "context_len = 2\n",
    "vocab_size = 3\n",
    "\n",
    "logits = torch.randn((batch_size,context_len,vocab_size))\n",
    "targets = torch.randint(0,vocab_size,(batch_size,context_len))\n",
    "\n",
    "print(logits)\n",
    "print(\"-----------------------------\")\n",
    "print(targets)\n",
    "\n",
    "print(targets.flatten())\n",
    "print(logits.flatten(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d393eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        # no data\n",
    "        return float(\"nan\")\n",
    "    elif num_batches == None:\n",
    "        # if num_batches is 0 then whole data\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(len(data_loader),num_batches)\n",
    "    \n",
    "\n",
    "    for i,(inp,target) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(inp, target, model, device)\n",
    "            total_loss = loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00372ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.003658294677734\n",
      "Validation loss: 10.96114730834961\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31317988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Train Loss 1.6445178985595703, Val Loss 6.760586738586426\n",
      "Epoch 0, Step 5, Train Loss 0.9228534698486328, Val Loss 6.630193710327148\n",
      "I want cake, I felt to go a little wild--I felt nervous and uncertain.  \"Once, when I looked up, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I couldn't look at that it. . . . . . . . . . . . . .  \"Oh, my own lair,\" he said.      \"Ah, I was growing like the _rose Dub\n",
      "Epoch 1, Step 10, Train Loss 0.5872092247009277, Val Loss 6.589439868927002\n",
      "Epoch 1, Step 15, Train Loss 0.3463591933250427, Val Loss 6.648420333862305\n",
      "I want cake, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!  He laughed again, and threw back his head to look up at the sketch of the donkey. \"strongest,\" I found her so. Gisburn--couldn't face it. But I had given up his painting because he had married her.   \"--that was what the women called it was not till after that event that the _rose Dub\n",
      "Epoch 2, Step 20, Train Loss 0.2820005714893341, Val Loss 6.759932041168213\n",
      "Epoch 2, Step 25, Train Loss 0.23800061643123627, Val Loss 6.87761116027832\n",
      "I want cake always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.) \"This is my own lair,\" he said, in all good faith, as if excusing herself. He shrugged his shoulders, still\n",
      "Epoch 3, Step 30, Train Loss 0.3002798855304718, Val Loss 6.939728736877441\n",
      "Epoch 3, Step 35, Train Loss 0.1569512039422989, Val Loss 7.010669708251953\n",
      "I want cake always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it were, the honour of the portrait. \"Jack is my own lair,\" he said. The plain the women called it. And it was therefore instructive to note what effect\n",
      "Epoch 4, Step 40, Train Loss 0.12101864814758301, Val Loss 7.06467866897583\n",
      "I want cake always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)  \"The height of his glory\"--that was what the women called it was not till after that event that the _rose Dub\n",
      "Epoch 5, Step 45, Train Loss 0.09333022683858871, Val Loss 7.149227142333984\n",
      "Epoch 5, Step 50, Train Loss 0.08305727690458298, Val Loss 7.1294636726379395\n",
      "I want cake, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!  Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: \"If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay.\"  Yes\n",
      "Epoch 6, Step 55, Train Loss 0.052465006709098816, Val Loss 7.198616981506348\n",
      "Epoch 6, Step 60, Train Loss 0.057189811021089554, Val Loss 7.256064414978027\n",
      "I want cake, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!  Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: \"If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn't let it stay.\"  Yes\n",
      "Epoch 7, Step 65, Train Loss 0.04581795260310173, Val Loss 7.325921058654785\n",
      "Epoch 7, Step 70, Train Loss 0.04620802029967308, Val Loss 7.356784820556641\n",
      "I want cake, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's past!  Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: \"If you stand here you can just manage to see it. I was _the_ fashionable painter.\"            Yes\n",
      "Epoch 8, Step 75, Train Loss 0.07660745084285736, Val Loss 7.388068199157715\n",
      "Epoch 8, Step 80, Train Loss 0.06814393401145935, Val Loss 7.432150840759277\n",
      "I want cake had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: \"Be dissatisfied with your leisure!\" as once one had longed to say: \"Be dissatisfied with your work!\"  But, with the cry on my lips, my diagnosis suffered an unexpected check.         His ridiculous modesty--Jack's the reason why I don't dabble any more, my dear Rick\n",
      "Epoch 9, Step 85, Train Loss 0.0456027127802372, Val Loss 7.334517478942871\n",
      "I want cake always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)   \"Mr. Rickham wanted to see it. I had sent all my traps in advance, and I had only to set\n"
     ]
    }
   ],
   "source": [
    "from train import train_model_simple\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "model.parameters(),\n",
    "lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "start_context=\"I want cake\", tokenizer=tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
